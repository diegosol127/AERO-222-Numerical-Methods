{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Random Noise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability density for the Gaussian distribution is\n",
    "\n",
    "$$p(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}}e^{\\frac{(x-\\mu)^2}{2 \\sigma^2}}$$\n",
    "\n",
    "where $\\mu$ is the mean and $\\sigma$ is the standard deviation. This function has its peak at the mean, and its \"spread\" increases with the standard deviation.\n",
    "\n",
    "Some of the problems on the homework expect the sampled data to be corrupted with Gaussian noise. You are provided with the parameters that define the distribution in the form\n",
    "\n",
    "$$\\mathcal{N}(\\mu,\\sigma)$$\n",
    "\n",
    "which we can easily implement in Python. For example, to generate $N$ data points with a normal distribution defined by $\\mathcal{N}(\\mu,\\sigma)$, use the following code:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "gauss_noise = np.random.normal(mu,sigma,N)\n",
    "```\n",
    "&nbsp;\n",
    "\n",
    "Refer to the [`numpy.random.normal` documentation page](https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html) for more information on generating samples from a normal (Gaussian) distribution."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Norms\n",
    "\n",
    "Vector norms are non-negative scalar values associated with a vector, usually used to describe the length or magnitude of a vector. The general form for the $p$-norm of a vector $x = (x_1,x_2,\\ldots,x_n)$ is\n",
    "\n",
    "$$\\lVert x \\rVert _p = \\left(\\sum_{i=1}^{n} {\\left| x_i \\right|^p}\\right)^{1/p}$$\n",
    "\n",
    "Below are some commonly used norms.\n",
    "\n",
    "### $L_1$ Vector Norm\n",
    "\n",
    "The $L_1$ norm is calculated as the sum of the absolute vector values. This norm is also called the taxicab or Manhattan norm since it can be visualized as the distance a cab drives through the streets of Manhattan (making only right-angle turns). It is given by\n",
    "\n",
    "$$\\lVert x \\rVert _1 = \\sum_{i=1}^{n} {\\left| x_i \\right|}$$\n",
    "\n",
    "### $L_2$ Vector Norm\n",
    "The most common norm is the $L_2$ or Euclidean norm. In 2-D and 3-D space, it measures as the distance of the shortest straight line that connects two points. It is computed as follows\n",
    "\n",
    "$$\\lVert x \\rVert _2 = \\left(\\sum_{i=1}^{n} {\\left| x_i \\right|^2}\\right)^{1/2}$$\n",
    "\n",
    "### $L_\\infty$ Vector Norm\n",
    "The $L_\\infty$ is defined by evaluating the $p$-norm in the limit as $p \\rightarrow \\infty$. It is also reffered to as the max norm because it is effectively the same as computing the maximum value of the vector. It is calculated with the equation\n",
    "\n",
    "$$\\lVert x \\rVert _\\infty = \\max{\\left(\\left| x_1 \\right|,\\left| x_2 \\right|,\\ldots,\\left| x_n \\right|\\right)}$$\n",
    "\n",
    "To learn how to leverage the `numpy` toolbox to compute vector norms, refer to the [`numpy.linalg.norm` documentation page](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted Linear Least-Squares\n",
    "\n",
    "Recall the original linear least-squares minimization problem. For a given set of data points, $[x_n,y_n]$,\n",
    "\n",
    "$$x = \\begin{bmatrix}\n",
    "    x_1 \\\\\n",
    "    x_2 \\\\\n",
    "    \\vdots \\\\\n",
    "    x_n\n",
    "\\end{bmatrix}, ~~~~~\n",
    "y = \\begin{bmatrix}\n",
    "    y_1 \\\\\n",
    "    y_2 \\\\\n",
    "    \\vdots \\\\\n",
    "    y_n\n",
    "\\end{bmatrix}$$\n",
    "\n",
    " and a fitting function, $\\hat{y}(x)$, consisting of $m$ linearly independent functions,\n",
    " \n",
    "$$\\hat{y}(x) = c_1 f_1(x) + c_2 f_2(x) + \\ldots + c_m f_m(x)$$\n",
    "\n",
    "we can say the following\n",
    "\n",
    "$$y \\approx \\hat{y} = Ac ~~~\\implies~~~\n",
    "\n",
    "\\begin{bmatrix}\n",
    "    y_1 \\\\\n",
    "    y_2 \\\\\n",
    "    \\vdots \\\\\n",
    "    y_n\n",
    "\\end{bmatrix}\n",
    "\n",
    "\\approx\n",
    "\n",
    "\\begin{bmatrix}\n",
    "    \\hat{y}_1 \\\\\n",
    "    \\hat{y}_2 \\\\\n",
    "    \\vdots \\\\\n",
    "    \\hat{y}_n\n",
    "\\end{bmatrix}\n",
    "\n",
    "=\n",
    "\n",
    "\\begin{bmatrix}\n",
    "    f_{1}(x_{1}) & f_{2}(x_{1}) & \\ldots & f_{m}(x_{1})\\\\\n",
    "    f_{1}(x_{2}) & f_{2}(x_{2}) & \\ldots & f_{m}(x_{2})\\\\\n",
    "    \\vdots       &      \\vdots  & \\ddots &       \\vdots\\\\\n",
    "    f_{1}(x_{n}) & f_{2}(x_{n}) & \\ldots & f_{m}(x_{n})\\\\\n",
    "\\end{bmatrix}\n",
    "\n",
    "\\begin{bmatrix}\n",
    "    c_{1} \\\\\n",
    "    c_{2} \\\\\n",
    "    \\vdots \\\\\n",
    "    c_{m} \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "where $c_k$ is the coefficient the scales $f_k(x)$, and $m < n$. The optimal solution to the minimization problem\n",
    "\n",
    "$$min \\sum_{k=1}^{n}\\left(y_{k} - \\hat{y}_{k}\\right)^{2}$$\n",
    "\n",
    "is given by the equation\n",
    "\n",
    "$$\\boxed{c = \\left(A^{T}A\\right)^{-1}A^{T}y}$$\n",
    "\n",
    "The method of ordinary least-squares assumes that there is constant covariance in the errors. In other words, the weight (importance) of each data point and its effect on the best-fit curve is equal. The weighted least-squares method breaks this assumption and assigns a specific weight to each data point, essentially assuming that some measurements are more accurate than others. This is done through a diagonal weighing matrix, $W$,\n",
    "\n",
    "$$W = \\begin{bmatrix}\n",
    "    w_1 & 0 & \\ldots & 0 \\\\\n",
    "    0 & w_2 & \\ldots & 0 \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    0 & 0 & \\ldots & w_n \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "The unequal weighing on the different measurements that result from applying this weight matrix in the linear least-squares problem forces the solution to prioritize accurate curve-fitting around the data points that were assigned a higher weight. This weighted linear least-squares problem is a modified version of the original least-squares optimization problem and is represented by\n",
    "\n",
    "$$min \\sum_{k=1}^{n} w_k \\left(y_{k} - \\hat{y}_{k}\\right)^{2}$$\n",
    "\n",
    "The solution to the weighted linear least-squares problem is\n",
    "\n",
    "$$\\boxed{c = \\left(A^{T} W A\\right)^{-1}A^{T} W y}$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
