{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving Systems of Nonlinear Equations\n",
    "\n",
    "Consider a system of $m$ nonlinear equations of the form\n",
    "\n",
    "$$\\begin{align}\n",
    "y_1 &= f_1(\\boldsymbol{x}) = 0 \\nonumber \\\\\n",
    "y_2 &= f_2(\\boldsymbol{x}) = 0 \\nonumber \\\\\n",
    "\\vdots \\nonumber \\\\\n",
    "y_m &= f_m(\\boldsymbol{x}) = 0 \\nonumber \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "where $\\boldsymbol{x} = [x_1, x_2, \\ldots, x_m]^T$ is the vector of inputs to the nonlinear equations. We wish to solve for the solution vector, $\\boldsymbol{x}$, that satisfies this system of equations. Let $\\boldsymbol{f}(\\boldsymbol{x}) = [f_1(\\boldsymbol{x}), f_2(\\boldsymbol{x}), \\ldots, f_m(\\boldsymbol{x})]^T$ be the vector of nonlinear equations.\n",
    "\n",
    "We start by expanding out the Taylor series to a first-order approximation. Let $\\bar{\\boldsymbol{x}}$ be the fixed point we expand the approximation about.\n",
    "\n",
    "$$0 = \\boldsymbol{f}(\\boldsymbol{x}) \\overset{\\Delta}= \\boldsymbol{f}(\\bar{\\boldsymbol{x}}) + \\nabla \\boldsymbol{f}(\\bar{\\boldsymbol{x}})(\\boldsymbol{x} - \\bar{\\boldsymbol{x}}) + \\mathcal{O}^2$$\n",
    "$$0 \\approx \\boldsymbol{f}(\\bar{\\boldsymbol{x}}) + \\nabla \\boldsymbol{f}(\\bar{\\boldsymbol{x}})(\\boldsymbol{x} - \\bar{\\boldsymbol{x}})$$\n",
    "\n",
    "We omit the higher-order terms and set the approximation equal to zero which will let us solve this problem linearly.\n",
    "\n",
    "$$0 = \\boldsymbol{f}(\\bar{\\boldsymbol{x}}) + \\nabla \\boldsymbol{f}(\\bar{\\boldsymbol{x}})(\\boldsymbol{x} - \\bar{\\boldsymbol{x}})$$\n",
    "\n",
    "Let's inspect the $\\nabla \\boldsymbol{f}(\\bar{\\boldsymbol{x}})$ term. $\\nabla$ is the gradiant operator which performs a vector derivative on its operand. Recall that both $\\boldsymbol{x}$ and $\\boldsymbol{f}(\\boldsymbol{x})$ are vector quantities.\n",
    "\n",
    "$$\\nabla \\boldsymbol{f}(\\bar{\\boldsymbol{x}}) = \\frac{\\partial \\boldsymbol{f}}{\\partial \\boldsymbol{x}}(\\bar{\\boldsymbol{x}}) = \n",
    "\\begin{bmatrix}\n",
    "    \\dfrac{\\partial f_1}{\\partial x_1}(\\bar{\\boldsymbol{x}}) & \\dfrac{\\partial f_1}{\\partial x_2}(\\bar{\\boldsymbol{x}}) & \\ldots & \\dfrac{\\partial f_1}{\\partial x_m}(\\bar{\\boldsymbol{x}}) \\\\[3ex]\n",
    "    \\dfrac{\\partial f_2}{\\partial x_1}(\\bar{\\boldsymbol{x}}) & \\dfrac{\\partial f_2}{\\partial x_2}(\\bar{\\boldsymbol{x}}) & \\ldots & \\dfrac{\\partial f_2}{\\partial x_m}(\\bar{\\boldsymbol{x}}) \\\\[1ex]\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\[1ex]\n",
    "    \\dfrac{\\partial f_m}{\\partial x_1}(\\bar{\\boldsymbol{x}}) & \\dfrac{\\partial f_m}{\\partial x_2}(\\bar{\\boldsymbol{x}}) & \\ldots & \\dfrac{\\partial f_m}{\\partial x_m}(\\bar{\\boldsymbol{x}}) \\\\\n",
    "\\end{bmatrix} = J(\\bar{\\boldsymbol{x}})$$\n",
    "\n",
    "Here, we define the Jacobian matrix, $J(\\bar{\\boldsymbol{x}})$. Each element of the this matrix represents the derivative of one of the nonlinear functions with respect to a specific input evaluated at the linearized point, $\\bar{\\boldsymbol{x}}$. We can substitute the Jacobian into the first-order approximation.\n",
    "\n",
    "$$0 = \\boldsymbol{f}(\\bar{\\boldsymbol{x}}) + J(\\bar{\\boldsymbol{x}})(\\boldsymbol{x} - \\bar{\\boldsymbol{x}})$$\n",
    "\n",
    "Simplify this expression further by letting $\\bar{f} = f(\\bar{\\boldsymbol{x}})$ and $\\bar{J} = J(\\bar{\\boldsymbol{x}})$.\n",
    "\n",
    "$$0 = \\bar{f} + \\bar{J}(\\boldsymbol{x} - \\bar{\\boldsymbol{x}})$$\n",
    "\n",
    "Rearranging this equation and solving for the solution, $\\boldsymbol{x}$, we get\n",
    "\n",
    "$$\\boldsymbol{x} = \\bar{\\boldsymbol{x}} - \\bar{J}^{-1} \\bar{f}$$\n",
    "\n",
    "Recall that $\\bar{\\boldsymbol{x}}$ represents a fixed-point, $\\bar{f} = f(\\boldsymbol{x})$, and $\\bar{J} = J(\\bar{\\boldsymbol{x}})$. As an iterative process, we are solving for $\\boldsymbol{x}_{k+1}$ given an initial guess, $\\boldsymbol{x}_k$. Here we define\n",
    "\n",
    "$$f_k = f(\\boldsymbol{x}_k)$$\n",
    "$$J_k = J(\\boldsymbol{x}_k)$$\n",
    "\n",
    "We now arrive at the solution to the iterative process for solving systems of nonlinear equations. Note the resemblance to the Newton-Raphson method we learned earlier in the semester which dealt with the one-dimensional (scalar) case. This equation represents the more general form for a system of arbitrary dimensions.\n",
    "\n",
    "$$\\boxed{\\boldsymbol{x}_{k+1} = \\boldsymbol{x}_k - J^{-1}_k \\boldsymbol{f}_k}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1\n",
    "\n",
    "Let's try an example. Consider the following system of equations:\n",
    "\n",
    "$$\\begin{align}\n",
    "f_1(\\boldsymbol{x}) &= x_1^3 + x_2 - 1 = 0 \\nonumber \\\\\n",
    "f_2(\\boldsymbol{x}) &= -x_1 + x_2^3 + 1 = 0\\nonumber\n",
    "\\end{align}$$\n",
    "\n",
    "Given the initial guess of $\\boldsymbol{x_0} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$, find the solution to this systems of nonlinear equations to a residual tolerance of $\\lVert \\boldsymbol{f}(\\boldsymbol{x}) \\rVert _2 < 10^{-8}$. Report the number of iterations, the residual, and the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE SOLUTION HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2\n",
    "\n",
    "Let's try another example. Consider the following system of equations:\n",
    "\n",
    "$$\\begin{align}\n",
    "f_1(\\boldsymbol{x}) &= 3x_1 + x_1^2 + x_2^2 = 0 \\nonumber \\\\\n",
    "f_2(\\boldsymbol{x}) &= x_1x_2 - x_2^2 = 0\\nonumber\n",
    "\\end{align}$$\n",
    "\n",
    "Given the initial guess of $\\boldsymbol{x_0} = \\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix}$, find the solution to this system of nonlinear equations to a residual tolerance of $\\lVert \\boldsymbol{f}(\\boldsymbol{x}) \\rVert _2 < 10^{-10}$. Report the number of iterations, the residual, and the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE SOLUTION HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonlinear Least-Squares Iterative Method\n",
    "\n",
    "Let $n > m$. For a given set of $n$ data points, $[x_n,y_n]$,\n",
    "\n",
    "$$\\boldsymbol{x} = \\begin{bmatrix}\n",
    "    x_1 \\\\\n",
    "    x_2 \\\\\n",
    "    \\vdots \\\\\n",
    "    x_n\n",
    "\\end{bmatrix}, ~~~~~\n",
    "\\boldsymbol{y} = \\begin{bmatrix}\n",
    "    y_1 \\\\\n",
    "    y_2 \\\\\n",
    "    \\vdots \\\\\n",
    "    y_n\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "a set of $m$ unknown parameters, $\\boldsymbol{c}$,\n",
    "\n",
    "$$\\boldsymbol{c} = \\begin{bmatrix}\n",
    "    c_1 \\\\\n",
    "    c_2 \\\\\n",
    "    \\vdots \\\\\n",
    "    c_m\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "and a nonlinear fitting function, $\\hat{\\boldsymbol{y}} = f(\\boldsymbol{x},\\boldsymbol{c})$, we want to find the solution that minimizes the $L_2$-norm of the residual, $\\boldsymbol{r} = \\hat{\\boldsymbol{y}} - \\boldsymbol{y}$. The least-squares minimization problem is given by\n",
    "\n",
    "$$\\min \\sum_{k=1}^{n}\\left(\\hat{y}(x_{k}) - y_{k}\\right)^{2}$$\n",
    "\n",
    "Unlike the classic linear least-squares problem, we cannot immediately convert the equation $\\hat{\\boldsymbol{y}} = f(\\boldsymbol{x},\\boldsymbol{c})$ into a linear form $\\boldsymbol{y} = A \\boldsymbol{x}$ since the parameters are embedded in the equation in a nonlinear fashion that cannot be simplified. To remedy this, we will start by considering the residual equation which is equal to zero in the most ideal case (we want to minimize it). Recall how we defined the residual\n",
    "\n",
    "$$\\boldsymbol{r} = \\hat{\\boldsymbol{y}} - \\boldsymbol{y}$$\n",
    "\n",
    "More explicitly, we write \n",
    "\n",
    "$$\\boldsymbol{r}(\\boldsymbol{x},\\boldsymbol{y},\\boldsymbol{c}) = f(\\boldsymbol{x},\\boldsymbol{c}) - \\boldsymbol{y} = 0$$\n",
    "\n",
    "Now we can linearize the residual function, $\\boldsymbol{r}(\\boldsymbol{x},\\boldsymbol{y},\\boldsymbol{c})$, about a fixed-point, $\\bar{\\boldsymbol{c}}$, by means of a first-order Taylor series expansion.\n",
    "\n",
    "$$0 = \\boldsymbol{r}(\\boldsymbol{x},\\boldsymbol{y},\\boldsymbol{c}) \\overset{\\Delta}= \\boldsymbol{r}(\\boldsymbol{x},\\boldsymbol{y},\\bar{\\boldsymbol{c}}) + \\nabla \\boldsymbol{r}(\\boldsymbol{x},\\boldsymbol{y},\\bar{\\boldsymbol{c}})(\\boldsymbol{c} - \\bar{\\boldsymbol{c}}) + \\mathcal{O}^2$$\n",
    "$$0 \\approx \\boldsymbol{r}(\\boldsymbol{x},\\boldsymbol{y},\\bar{\\boldsymbol{c}}) + \\nabla \\boldsymbol{r}(\\boldsymbol{x},\\boldsymbol{y},\\bar{\\boldsymbol{c}})(\\boldsymbol{c} - \\bar{\\boldsymbol{c}})$$\n",
    "\n",
    "We omit the higher-order terms and set this approximation equal to zero which will let us solve this problem linearly.\n",
    "\n",
    "$$0 = \\boldsymbol{r}(\\boldsymbol{x},\\boldsymbol{y},\\bar{\\boldsymbol{c}}) + \\nabla \\boldsymbol{r}(\\boldsymbol{x},\\boldsymbol{y},\\bar{\\boldsymbol{c}})(\\boldsymbol{c} - \\bar{\\boldsymbol{c}})$$\n",
    "\n",
    "Let's quickly inspect the gradient of the residual to see how we can simplify the expression.\n",
    "\n",
    "$$\\begin{align}\n",
    "\\nabla \\boldsymbol{r}(\\boldsymbol{x},\\boldsymbol{y},\\bar{\\boldsymbol{c}}) &= \\nabla \\left( f(\\boldsymbol{x},\\bar{\\boldsymbol{c}}) - \\boldsymbol{y} \\right) \\nonumber \\\\\n",
    "&= \\nabla f(\\boldsymbol{x},\\bar{\\boldsymbol{c}}) - \\nabla \\boldsymbol{y} \\nonumber \\\\\n",
    "&= \\nabla f(\\boldsymbol{x},\\bar{\\boldsymbol{c}}) \\nonumber\n",
    "\\end{align}$$\n",
    "\n",
    "Therefore, we can rewrite the linear approximation as\n",
    "\n",
    "$$0 = \\boldsymbol{r}(\\boldsymbol{x},\\boldsymbol{y},\\bar{\\boldsymbol{c}}) + \\nabla f(\\boldsymbol{x},\\bar{\\boldsymbol{c}}) (\\boldsymbol{c} - \\bar{\\boldsymbol{c}})$$\n",
    "\n",
    "Similar to our earlier discussion on solving systems of nonlinear equations, $\\nabla f(\\boldsymbol{x},\\bar{\\boldsymbol{c}})$ will return a Jacobian matrix. Note that for this problem, the unknown parameters are in the vector $\\boldsymbol{c}$ (not $\\boldsymbol{x}$, which contains the input data points). This implies that our gradient is with respect to the coefficients, $\\boldsymbol{c}$. In other words, $\\nabla = \\dfrac{\\partial}{\\partial \\boldsymbol{c}}$. In the case of the nonlinear least-squares problem, the Jacobian matrix will have the form,\n",
    "\n",
    "$$\\nabla f(\\boldsymbol{x},\\bar{\\boldsymbol{c}}) = \\frac{\\partial f}{\\partial \\boldsymbol{c}}(\\boldsymbol{x},\\bar{\\boldsymbol{c}}) = \n",
    "\\begin{bmatrix}\n",
    "    \\dfrac{\\partial f}{\\partial c_1}(x_1,\\bar{\\boldsymbol{c}}) & \\dfrac{\\partial f}{\\partial c_2}(x_1,\\bar{\\boldsymbol{c}}) & \\ldots & \\dfrac{\\partial f}{\\partial c_m}(x_1,\\bar{\\boldsymbol{c}}) \\\\[3ex]\n",
    "    \\dfrac{\\partial f}{\\partial c_1}(x_2,\\bar{\\boldsymbol{c}}) & \\dfrac{\\partial f}{\\partial c_2}(x_2,\\bar{\\boldsymbol{c}}) & \\ldots & \\dfrac{\\partial f}{\\partial c_m}(x_2,\\bar{\\boldsymbol{c}}) \\\\[1ex]\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\[1ex]\n",
    "    \\dfrac{\\partial f}{\\partial c_1}(x_n,\\bar{\\boldsymbol{c}}) & \\dfrac{\\partial f}{\\partial c_2}(x_n,\\bar{\\boldsymbol{c}}) & \\ldots & \\dfrac{\\partial f}{\\partial c_m}(x_n,\\bar{\\boldsymbol{c}}) \\\\\n",
    "\\end{bmatrix} = J(\\boldsymbol{x},\\bar{\\boldsymbol{c}})$$\n",
    "\n",
    "Note that the rows correspond to the $n$ data points, $\\boldsymbol{x}$, and the columns correspond to the partial derivative of the fitting function with respect to the $m$ unknown parameters, $\\boldsymbol{c}$. We can substitute the Jacobian into the first order approximation and let $\\Delta \\boldsymbol{c} = (\\boldsymbol{c} - \\bar{\\boldsymbol{c}})$ to get\n",
    "\n",
    "$$0 = \\boldsymbol{r}(\\boldsymbol{x},\\boldsymbol{y},\\bar{\\boldsymbol{c}}) + J(\\boldsymbol{x},\\bar{\\boldsymbol{c}}) \\Delta \\boldsymbol{c}$$\n",
    "\n",
    "Simplify this expression further by letting $\\bar{\\boldsymbol{r}} = \\boldsymbol{r}(\\boldsymbol{x},\\boldsymbol{y},\\bar{\\boldsymbol{c}})$ and $\\bar{J} = J(\\boldsymbol{x},\\bar{\\boldsymbol{c}})$\n",
    "\n",
    "$$0 = \\bar{\\boldsymbol{r}} + \\bar{J} \\Delta \\boldsymbol{c}$$\n",
    "\n",
    "Recall that the vector $\\boldsymbol{c}$ contains the unknown parameters we wish to solve for. It is embedded in the term $\\Delta \\boldsymbol{c} = (\\boldsymbol{c} - \\bar{\\boldsymbol{c}})$. Therefore, if we can solve $\\Delta \\boldsymbol{c}$, we can extract the solution $\\boldsymbol{c}$. Rearrange this equation to get it into a form that can be solved with the linear-least squares solution.\n",
    "\n",
    "$$\\boxed{-\\bar{\\boldsymbol{r}} = \\bar{J} \\Delta \\boldsymbol{c}} \\implies \\boldsymbol{y} = A \\boldsymbol{x}$$\n",
    "\n",
    "From previous lessons, we already know the solution to this problem so we can solve for the difference vector, $\\Delta \\boldsymbol{c}$, with the equation\n",
    "\n",
    "$$\\Delta \\boldsymbol{c} = -\\left(\\bar{J}^T \\bar{J}\\right)^{-1} \\bar{J}^T \\bar{\\boldsymbol{r}}$$\n",
    "\n",
    "Substituting for $\\Delta \\boldsymbol{c} = (\\boldsymbol{c} - \\bar{\\boldsymbol{c}})$ and rearranging the equation, solve for the unknown parameters, $\\boldsymbol{c}$.\n",
    "\n",
    "$$\\boldsymbol{c} - \\bar{\\boldsymbol{c}} = -\\left(\\bar{J}^T \\bar{J}\\right)^{-1} \\bar{J}^T \\bar{\\boldsymbol{r}}$$\n",
    "$$\\boldsymbol{c} = \\bar{\\boldsymbol{c}} - \\left(\\bar{J}^T \\bar{J}\\right)^{-1} \\bar{J}^T \\bar{\\boldsymbol{r}}$$\n",
    "\n",
    "Recall that $\\bar{\\boldsymbol{c}}$ represents a fixed-point, $\\bar{J} = J(\\boldsymbol{x},\\bar{\\boldsymbol{c}})$, and $\\bar{\\boldsymbol{r}} = \\boldsymbol{r}(\\boldsymbol{x},\\boldsymbol{y},\\bar{\\boldsymbol{c}})$. As an iterative process, we are solving for $\\boldsymbol{c}_{k+1}$ given an initial guess, $\\boldsymbol{c}_k$. Here we define\n",
    "\n",
    "$$J_k = J(\\boldsymbol{x},\\boldsymbol{c}_k)$$\n",
    "$$\\boldsymbol{r}_k = \\boldsymbol{r}(\\boldsymbol{x},\\boldsymbol{y},\\boldsymbol{c}_k)$$\n",
    "\n",
    "We now arrive at the solution to the iterative nonlinear least-squares problem\n",
    "\n",
    "$$\\boxed{\\boldsymbol{c}_{k+1} = \\boldsymbol{c}_k - \\left(J_k^T J_k\\right)^{-1} J_k^T \\boldsymbol{r}_k}$$\n",
    "\n",
    "This method converges under the following error and residual criteria\n",
    "\n",
    "$$\\begin{align}\n",
    "\\text{Error Criteria: } \\lVert \\boldsymbol{c}_{k+1} \\rVert &< \\lVert \\boldsymbol{c}_{k} \\rVert \\nonumber \\\\\n",
    "\\text{Residual Criteria: } \\lVert \\boldsymbol{r}_{k+1} \\rVert &< \\lVert \\boldsymbol{r}_{k} \\rVert \\implies \\lVert \\boldsymbol{f}_{k+1} - \\boldsymbol{y} \\rVert < \\lVert \\boldsymbol{f}_{k} - \\boldsymbol{y} \\rVert \\implies \\lVert \\boldsymbol{f}_{k+1} \\rVert < \\lVert \\boldsymbol{f}_{k} \\rVert \\nonumber\n",
    "\\end{align}$$\n",
    "\n",
    "where $\\boldsymbol{f}_k = f(\\boldsymbol{x}, \\boldsymbol{c}_k)$. When asked to iterate this method to a specified tolerance, $\\varepsilon$, this criteria is expressed as\n",
    "\n",
    "$$\\begin{align}\n",
    "\\text{Error Criteria: } \\lVert \\boldsymbol{c}_{k+1} - \\boldsymbol{c}_{k}\\rVert &< \\varepsilon \\nonumber \\\\\n",
    "\\text{Residual Criteria: } \\lVert \\boldsymbol{r}_{k+1} - \\boldsymbol{r}_{k} \\rVert &< \\varepsilon \\implies \\lVert \\boldsymbol{f}_{k+1} - \\boldsymbol{f}_{k} \\rVert < \\varepsilon \\nonumber\n",
    "\\end{align}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Create 100 data points for $x$ uniformly distributed between $-1.5$ and $1.5$ with the equation\n",
    "\n",
    "$$f(x) = \\beta_1 x^3 + \\beta_2 e^{\\beta_3 x} + \\beta_4 x \\sin{x^2}$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\beta =\n",
    "\\begin{bmatrix}\n",
    "    2 \\\\\n",
    "    4 \\\\\n",
    "    -0.5 \\\\\n",
    "    -\\pi/2\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Corrupt the measurments with Gaussian noise $\\mathcal{N}(0,0.1)$. Estimate the parameters by iterative nonlinear least-squares with initial guess $\\beta = [1,1,1,1]^T$ to an $L_2$-normed residual tolerance of $1 \\times 10^{-5}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE SOLUTION HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE SOLUTION HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE SOLUTION HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aero222",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
